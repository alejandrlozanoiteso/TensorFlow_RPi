{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuClass": "premium"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow"
      ],
      "metadata": {
        "id": "mPcvDe0wtCTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.12.0"
      ],
      "metadata": {
        "id": "D_FXaOictACa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 'audios.zip' -d audios/"
      ],
      "metadata": {
        "id": "fBy8Jwes6Z24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo de visualización de audio"
      ],
      "metadata": {
        "id": "3z7JUyqQIR_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.logging_ops import audio_summary\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_audio(filepath):\n",
        "\n",
        "  #Cargar el archivo binario\n",
        "  audio_binary=tf.io.read_file(filepath)\n",
        "\n",
        "  # Decodificar el contenido del archivo binario\n",
        "  # Secuencia de escalares (magnitudes) y frecuencia de muestreo\n",
        "  audio, audioSR = tf.audio.decode_wav(audio_binary)\n",
        "  #print(audio.shape)\n",
        "\n",
        "  # Trabajaremos con un eje\n",
        "  audio = tf.squeeze(audio, axis=-1)\n",
        "\n",
        "  t = np.arange(0, len(audio))\n",
        "  plt.plot(t, audio, 'g')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "wQdq3e6uIIop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath='/content/audios/on/on_0.wav'\n",
        "plot_audio(filepath)"
      ],
      "metadata": {
        "id": "356knvi8IP4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reproducción de audio"
      ],
      "metadata": {
        "id": "Yt_VOzj1h-GP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio, display\n",
        "\n",
        "def play_audio(filepath):\n",
        "\n",
        "  #Cargar el archivo binario\n",
        "  audio_binary=tf.io.read_file(filepath)\n",
        "\n",
        "  # Decodificar el contenido del archivo binario\n",
        "  # Secuencia de escalares (magnitudes) y frecuencia de muestreo\n",
        "  audio, audioSR = tf.audio.decode_wav(audio_binary)\n",
        "\n",
        "  audio = tf.squeeze(audio, axis=-1)\n",
        "  sr = tf.get_static_value(audioSR)\n",
        "  display(Audio(audio, rate=audioSR, autoplay=False))\n"
      ],
      "metadata": {
        "id": "CIuH9EQyiR-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "play_audio('/content/audios/on/on_0.wav')\n",
        "\n"
      ],
      "metadata": {
        "id": "yTX4nd0Ii7pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uso de STFT para obtener los espectros de frecuencia y magnitudes"
      ],
      "metadata": {
        "id": "Gu3yVJKxjrSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def audioToTensor(filepath):\n",
        "  #Cargar el archivo binario\n",
        "  audio_binary=tf.io.read_file(filepath)\n",
        "\n",
        "  # Decodificar el contenido del archivo binario\n",
        "  # Secuencia de escalares (magnitudes) y frecuencia de muestreo\n",
        "  audio, audioSR = tf.audio.decode_wav(audio_binary)\n",
        "\n",
        "  audio = tf.squeeze(audio, axis=-1)\n",
        "  sr = tf.get_static_value(audioSR)\n",
        "\n",
        "  spectro = tf.signal.stft(audio, frame_length=512, frame_step=128)\n",
        "  spectro = tf.abs(spectro)\n",
        "  return spectro"
      ],
      "metadata": {
        "id": "_wiOnxTDjznx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = audioToTensor('/content/audios/off/off_0.wav')\n",
        "print(s.shape)"
      ],
      "metadata": {
        "id": "BS5HpsGKmUCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizar el espectrograma"
      ],
      "metadata": {
        "id": "I7kXg1wqm_P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_spectrogram(spectrogram):\n",
        "  h = spectrogram.shape[0]\n",
        "  w = spectrogram.shape[1]\n",
        "  spec_log = np.log(tf.transpose(spectrogram) + np.finfo(float).eps)\n",
        "  plt.xlabel(\"Tiempo\")\n",
        "  plt.ylabel(\"Frecuencia\")\n",
        "  plt.pcolormesh(range(h), range(w), spec_log)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "V9DMttRum0pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "69qaBR9fhwVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_spectrogram(s)"
      ],
      "metadata": {
        "id": "16Sm1RRToOBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear generador de datos"
      ],
      "metadata": {
        "id": "yBU94za7wOj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear las listas archivos de audio y su etiqueta correspondiente\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def getExamples(datafolder):\n",
        "  X_audio = []\n",
        "  Y_command = []\n",
        "\n",
        "  # Comandos\n",
        "  commands = [os.path.basename(x) for x in glob.glob(datafolder + '*')]\n",
        "  print(commands)\n",
        "\n",
        "  for i, command in enumerate(commands):\n",
        "    for file in glob.glob(os.path.join(datafolder, command) + '/*.wav'):\n",
        "      X_audio.append(file)\n",
        "      Y_command.append(np.array(to_categorical(i, num_classes=len(commands))))\n",
        "  return np.asarray(X_audio), np.asarray(Y_command)"
      ],
      "metadata": {
        "id": "EFw9CF1GoPmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datafolder= '/content/audios/'\n",
        "X_audio, Y_command = getExamples(datafolder)"
      ],
      "metadata": {
        "id": "tuNZgNw_w7mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_audio), len(Y_command))"
      ],
      "metadata": {
        "id": "JfwFxRjhFZ_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_audio, X_audio_test, Y_command, Y_command_test = train_test_split(X_audio, Y_command, test_size=0.25)\n",
        "print(len(X_audio))\n",
        "print(len(Y_command))"
      ],
      "metadata": {
        "id": "ve0u0EVVHLFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementar el generador de Datos\n",
        "\n",
        "class MySequence(tf.keras.utils.Sequence):\n",
        "\n",
        "  def __init__(self, x_audio, y_command, batch_size):\n",
        "    self.x_audio = x_audio\n",
        "    self.y_command = y_command\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_audio)//self.batch_size\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    batch_y = self.y_command[idx * self.batch_size : (idx+1)*self.batch_size]\n",
        "    batch_x = np.zeros((self.batch_size, s.shape[0], s.shape[1]))\n",
        "    for i in range(0, self.batch_size):\n",
        "      batch_x[i] = audioToTensor(self.x_audio[idx * self.batch_size + i])\n",
        "    return batch_x, batch_y\n",
        "\n"
      ],
      "metadata": {
        "id": "-aPdmhH2IDa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar la forma de los datos de entrada y la salida esperada\n",
        "\n",
        "mS=MySequence(X_audio, Y_command, 16)\n",
        "i=iter(mS)\n",
        "bx, by = next(i)\n",
        "print(bx.shape, by.shape)"
      ],
      "metadata": {
        "id": "vw7d0pAXhZuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de modelo basado en redes neuronales recurrentes\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, BatchNormalization, Reshape\n",
        "from tensorflow.keras.layers import MaxPooling2D, Dropout, Flatten, Resizing\n",
        "\n",
        "input_tensor = Input(shape=(247, 257))\n",
        "x = Reshape((247, 257, 1))(input_tensor)\n",
        "x = Resizing(32,32)(x)\n",
        "x = BatchNormalization()(x)\n",
        "#x = Reshape((x.shape[1], x.shape[2], 1))(x)\n",
        "x = Conv2D(32, 3, activation='relu')(x)\n",
        "x = Conv2D(64, 3, activation='relu')(x)\n",
        "x = MaxPooling2D()(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output_tensor = Dense(4, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=input_tensor, outputs=output_tensor)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "bYomMKO3MsKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "5UldH5y0y0ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(model, to_file=\"model.png\", show_shapes=True)"
      ],
      "metadata": {
        "id": "LMpkKtN-o7mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "epochs = 5\n",
        "h = model.fit(MySequence(X_audio, Y_command, batch_size),\n",
        "              shuffle=True,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=MySequence(X_audio_test, Y_command_test, batch_size))"
      ],
      "metadata": {
        "id": "hAlLrGAVM2tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reconocimiento de comandos"
      ],
      "metadata": {
        "id": "l3SLq1rK8Fpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = audioToTensor('/content/audios/on/on_0.wav')\n",
        "tensor=tf.reshape(tensor, (1,247, 257,1))\n",
        "print(tensor.shape)"
      ],
      "metadata": {
        "id": "h4IRYI-27Wv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datafolder='/content/audios/'\n",
        "commands = [os.path.basename(x) for x in glob.glob(datafolder + '*')]\n",
        "\n",
        "#Predicción\n",
        "r = model.predict(tensor)\n",
        "print(r[0])\n",
        "print(commands[r[0].argmax()])"
      ],
      "metadata": {
        "id": "3vQwAIgC8EvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardar, cargar y verificar el modelo"
      ],
      "metadata": {
        "id": "cLHwLhFN_jYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gqllTRzBz50Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/modelAudioRecognition.h5')"
      ],
      "metadata": {
        "id": "SUQDSNIdRIk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "smodel=load_model('/content/modelAudioRecognition.h5')"
      ],
      "metadata": {
        "id": "zwGMBOTgniO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(smodel)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('audio_cnn.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "id": "8bxQaLrYnkFX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}